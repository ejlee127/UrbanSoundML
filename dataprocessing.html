<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Urban Sound Classification</title>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

    <!-- Our CSS -->
    <link rel="stylesheet" type="text/css" href="static/css/style.css">
   
  </head>

  <body >
    <!-- Start of container -->
    <div id="container-fluid">
      <hr>
      <div class="row"  id="top-row">
        <h3>Urban Sound Classification using Machine Learning:
          Data Processing</h3>
        <p>  </p>             
      </div>
      <hr>
      <div class = "row" id="top-row">        
        <div class="col-md-9">
          <div class="row">
            <div>
              The Urban Sound dataset provides 8,732 audio files formatted as .wav, in total 7GB.
              <img class="metadata-img" src="images/metadata_table.png">

              According to the authors, the audio files were collected from
              <a href="https://freesound.org/browse/"> freesound.org</a>, where 
              the source of each audio file come from and it is recorded in 
              the 'fsID' column in the metadata table. The duration of each audio can be
              obtained from 'end' - 'start', which gives maximum 4 second. The gun shot sounds are shortest.
            </div> 
            <div>
              To carry out the machine learning, we need input and algorithm. Can we use
              the wav files as input? Even without knowing the audio features, immediate answer is 'maybe not' because of the file size.
              The running time for training will be very long. 
              Then, what and how to extract from the wav files? 
            </div>
          </div>  
        </div>
        <div class="col-md-2">
          <div>
            <a href="index.html" class="btn btn-primary btn-block" role="button">Home</a>
            <a href="experiment.html" class="btn btn-primary btn-block" role="button">Experiment</a>
            <a href="result.html" class="btn btn-primary btn-block" role="button">Analysis</a>
            <a href="resources.html" class="btn btn-primary btn-block" role="button">Resources</a>
          </div>
        </div>
      </div>
      <div class = "row">        
        <div class="card card-big">
          <h4 class="card-header"> Audio data vs. MFCC</h4>
          <div class="card-body">
            <p class="card-text"> 
              We took the dataset and ran it through librosa libraries 
              to convert the wav files into MFCC files.  
              The Mel-Frequency Cepstral Coefficients (MFCC) 
              is a way of capturing the spectrum of the voice (phoneme) 
              so that it can used in voice recognition and machine learning.
            </p>
            <div>
              <pre class="code-box">
  import librosa.display
  import matplotlib.pyplot as plt
  import IPython.display as ipd

  def display_wav(signal,fn):
    librosa.display.waveplot(signal, sr=sr)
    plt.xlabel("Time")
    plt.ylabel("Amplitude")
    plt.savefig(fn, Bbox='tight')
    plt.show()
              </pre>
            </div>      
            <p> The following graphs and sounds show how much MFCC contains the original data.</p>
            <div class="card-group">
              <div class="card">
                <div class="card-body">
                  <h5 class="card-title"> Dog_bark: orginal </h5>
                  <div class="card-text">
                    <div class="code-box">
                      <code>
                        signal, sr = librosa.load("7383-3-0-0.wav", sr=22050)<br>
                        display_wav(signal, '../images/dog_bark_plot.png')
                      </code>
                    </div>
                  </div>
                </div>
                <div class="align-center">
                  <audio controls>
                    <source src="audio/7383-3-0-0.wav", type="audio/wav">
                  </audio>
                </div>
                <img src="images/dog_bark_plot.png" class="card-img-bottom wav2-img" alt="Card image cap">
              </div>              
              <div class="card">
                <div class="card-body">
                  <h5 class="card-title"> Dog_bark: reversed </h5>
                  <div class="card-text">
                    <div class="code-box">
                      <code>
                        import soundfile as sf<br>
                        mfccs = librosa.feature.mfcc(signal,sr=sr,n_mfcc=13)<br>
                        wav = librosa.feature.inverse.mfcc_to_audio(mfccs)<br>
                        display_wav(wav, '../images/dog_bark_reversed.png')<br>
                      </code>
                    </div>
                  </div>
                </div>
                <div class="align-center">
                  <audio controls>
                    <source src="audio/reversed.wav", type="audio/wav">
                  </audio>
                </div>
                <img src="images/dog_bark_reversed.png" class="card-img-bottom wav2-img" alt="Card image cap">
              </div>              
            </div>

          </div>
        </div>
      </div>
      <div class="row">
        <div class="card card-big">
          <h4 class="card-header"> MFCC data</h4>
          <div class="card-body">
            The returned MFCC from librosa.feature.mfcc() is a two-dimensional array of integers.
          </div>
          <div>
            <img src="images/dog_bark_mfcc_13.png" align="left">
            In the image, the vertical height is determined by the number of coefficients of MFC(= n_mfcc)
            and the width is determined by the sample rate(sr) and duration. The left image has n_mfcc=13 as height,
            and 173 as width. Each small square inside represents a number(amplitude).
            The returned MFCC has, in this example, a two-dimensional array of 13 by 173.
          </div>
        </div>
      </div>
      <div class="row">
        <div class="col-md-11">
          <p class="p-tiny bottom"> Eunjeong Lee, ejlee127 at gmail dot com, last updated in Nov. 2020</p>
        </div>
      </div>
    </div>
    <!-- End of container  -->
    
    
    <!-- d3 JavaScript -->
    <script src="https://d3js.org/d3.v4.min.js"></script>
    <!--<script src="static/js/app.js"></script>-->

    <!-- Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    
  </body>
</html>